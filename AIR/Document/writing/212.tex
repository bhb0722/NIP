
%%%%%%%%%%%%%%%%%%%%%%% file typeinst.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is the LaTeX source for the instructions to authors using
% the LaTeX document class 'llncs.cls' for contributions to
% the Lecture Notes in Computer Sciences series.x
% http://www.springer.com/lncs       Springer Heidelberg 2006/05/04
%
% It may be used as a template for your own input - copy it
% to a new file with a new name and use it as the basis
% for your article.
%
% NB: the document class 'llncs' has its own and detailed documentation, see
% ftp://ftp.springer.de/data/pubftp/pub/tex/latex/llncs/latex2e/llncsdoc.pdf
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[runningheads,a4paper]{llncs}

\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage{amsmath,ulem,epsfig,subfigure}

\usepackage{url}
\urldef{\mailsa}\path|goldencrosser@gmail.com|
\urldef{\mailsb}\path|rmkil@skku.edu|
\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}

\renewcommand\baselinestretch{1.0}

\begin{document}

\mainmatter  % start of an individual contribution

% first the title is needed
\title{Stock Price Prediction Based on a Network\protect\\
       with Gaussian Kernel Functions}

% a short form should be given in case it is too long for the running head
\titlerunning{Lecture Notes in Computer Science: Authors' Instructions}

% the name(s) of the author(s) follow(s) next
%
% NB: Chinese authors should write their first names(s) in front of
% their surnames. This ensures that the names appear correctly in
% the running heads and the author index.
%
\author{Dong Kyu Kim and Rhee Man Kil$^*$}
%
\authorrunning{Lecture Notes in Computer Science: Authors' Instructions}
% (feature abused for this document to repeat the title also on left hand pages)

% the affiliations are given next; don't give your e-mail address
% unless you accept that it will be published
\institute{College of Information and Communication Engineering, Sungkyunkwan Univesity\\
2066, Seobu-ro, Jangan-gu, Suwon, Gyeonggi-do, 440-746, Korea\\
\mailsa, \mailsb}
%\url{http://www.springer.com/lncs}}

%
% NB: a more complex sample for affiliations and the mapping to the
% corresponding authors can be found in the file "llncs.dem"
% (search for the string "\mainmatter" where a contribution starts).
% "llncs.dem" accompanies the document class "llncs.cls".
%

\titlerunning{Stock Price Prediction}
\authorrunning{D. Kim and R. Kil}

\maketitle

\begin{abstract}
This paper presents a new method of stock price prediction based on the phase space analysis
for stock price series. For the prediction model, a network with Gaussian kernel functions is selected
and this network is optimized by using a noise variance estimate.
As a result, the proposed model provides the high accuracy of predicted values.
Through the simulation for the prediction of KOSPI 200 stock price values, the effectiveness
of the proposed prediction model has been demonstrated.
\keywords{stock price prediction, regression model, nonparametric estimation, noise variance}
\end{abstract}

\section{Introduction}
It has been known that stock price values are generated by a nonlinear and non-stationary dynamic system according to
the trend of market which is associated with economic situation. For the problem of time series prediction,
the linear regression models such as ARMA and ARIMA models are usually used. However, these models
may not provide good estimates of predicted stock price values. If the stock price values are complete random, it is not possible to
predict the future values. However, in many cases, the stock price values are not completely random. Rather, in most cases,
they show coherent or sometimes chaotic behavior \cite{peter}.
In this case, stock price values can be described by a nonlinear dynamics. In this respect,
the nonlinear and nonparametric regression model is investigated to provide the better estimates of predicted values.
As a nonlinear regression model, a network with Gaussian kernel functions is considered since this model is
a nonparametric estimation model and
good for incremental learning due to the locality of kernel functions. Here, the problems are to determine the input structure
for the given prediction of stock price values and also to determine the proper size of regression models for the given stock price series.
For this purpose, the input structure is investigated by analyzing the phase space of stock price dynamics and
the proper size of regression models is investigated using the estimation of noise variances embedded in stock price series.
As a result, the proposed model provides an accurate estimation of predicted values. To demonstrate the effectiveness of
the proposed model, the Korea Composite Stock Price Index (KOSPI) 200 stock price series from June 2008 to June 2012 is used
for the prediction of stock price values. The proposed model can also be applied to other cases of time series prediction since  our model
is not just restricted to the problem of stock price prediction.

\section{Phase Space Analysis of Stock Price Series}

The goal of time series prediction is to predict future values from
a series of past values observed at regular time intervals. For the modeling of time series data, we usually rely
on the theory called the delay coordinate embedding proposed by
Packard et al. \cite{Packard80} and Takens \cite{Takens81}.  Their
purpose is to find the appropriate dimension of attractors generated
by the dynamical system.  To describe the dynamical system, let us
assume that there exists a $D$ dimensional state vector
$\vec{x}(t)$, $t \in \bbbr$; that is,
\begin{equation}
\vec{x}(t) = [x(t), x^{(1)}(t), \cdots, x^{(D-1)}(t)]^T
\end{equation}
where $x^{(i)}(t)$ represents the $i$th derivative of $x(t)$ with
respect to $t$.  This vector can be produced by a dynamical system,
\begin{equation}
\dot{\vec{x}} = \vec{F}(\vec{x}, t)
\end{equation}
where $\vec{F}$ represents the $D$ dimensional mapping function from
the state vector $\vec{x}$ to the vector $\dot{\vec{x}}$ at time
$t$.  With no information on the state space (or phase space), we can only observe
$x(t)$.  Moreover, we usually consider the sampled discrete time
series data $x(t_k)$; that is,
\begin{equation}
x(t_k) = x(t_0 + k\Delta t) \equiv x(k), \; \; k= 0, 1, 2, \cdots
\end{equation}
where $t_0$, $k$, and $\Delta t$ represent the initial time,
sampling step, and sampling interval, respectively.  Here, let us
construct $E$-dimensional vectors
\begin{equation} \label{delayCoordinate}
\vec{x}_{\tau,E}(k) = [x(k), x(k-\tau),\ldots,x(k-(E-1)\tau)]^T, \;
\; k= (E-1)\tau, \cdots
\end{equation}
where $\tau$ and $E$ represent the delay time measured as the unit of $\Delta t$ and embedding
dimension, respectively.

For the prediction of future values, we assume that the state
vectors in the reconstructed state space are governed by a nonlinear
function $f$; that is,
\begin{equation} \label{target:disc}
x(t_k+P) = f(\vec{x}_{\tau,E}(k))
\end{equation}
where $P$ represents the prediction step. Then, our goal is to make a
prediction model $\hat{f}$ estimating the unknown target function
$f$. However, since the geometric structure of $f$ is determined by
the embedding parameters $\tau$ and $E$, the performance of the
prediction model is strongly affected by the choice of the embedding
parameters. One of the typical methods is to choose the embedding
dimension $E$ by estimating the correlation dimension of the system
\cite{Havstad89}, and to choose the delay time $\tau$ at which the
first minimum of the mutual information occurs.
However, the correlation dimension does not indicate any information on the
target function $f$.  In this context, we consider to determine the
embedding parameters using the measure related to the smoothness of
the target function.

For the analysis of time series data, let us denote the $r$th
nearest neighbor of the vector $\vec{x}_{\tau,E}(k)$ by
$\vec{x}^{r}_{\tau,E}(k)$. Since the difficulty of estimation
depends on the smoothness of the target function $f$, we define the
gradient of $f$ at each point $\vec{x}_{\tau,E}(k)$ by
    \begin{equation} \label{gradient}
        \Delta f(\vec{x}_{\tau, E}^r) =
        \frac{ \left| f(\vec{x}_{\tau,E}(k))
        - f(\vec{x}^{r}_{\tau,E}(k)) \right| }
        {||\vec{x}_{\tau,E}(k) - \vec{x}^{r}_{\tau,E}(k)||},
    \end{equation}
where the norm in the denominator represents the Euclidian distance
in $\bbbr^E$. In this paper, $r=1$ is used; that is, the nearest
neighbor of the state vector.  Here, the smoothness measure
$S(\tau,E)$ of a target function $f$ \cite{Kil99}
is defined by using an
average of the gradient values for time series $x(t_k), \; \; k= 0, 1, \cdots, n-1$; that is,
\begin{equation}
        S(\tau,E) = 1- \frac{1}{n-(E-1)\tau}
        \sum^{n-1}_{k=(E-1)\tau} \Delta f(\vec{x}_{\tau,E}^1(k)).
\label{smooth}
\end{equation}

The smoothness measure $S(\tau,E)$ is closely related to the unfolding
        of the orbits of the vectors in the reconstructed state space. If
        the embedding dimension $E$ is too small, the state space is
        yet restricted to the low dimensional space. This
        restriction makes different orbits of vectors to be folded
        and placed closely to each other; that is, even the nearest neighbor
        vectors might have come from different orbits. Since the vectors
        on the different orbits are likely to have largely
        different target function values, large
        gradient values imply that the embedding dimension is too
        small. On the other hand, if the embedding dimension $E$ is
        large enough, the nearest neighbor vectors are obtained from
        the same orbit, which results in the small gradient values.
        Also, the smoothness measure $S(\tau,E)$ will not be much influenced by
        changes in $E$ when $E$ is large, because the state space is already
        unfolded.
From these observations, the optimal embedding dimension and delay
time can be determined by identifying the points where the smoothness measure
$S(\tau,E)$ changes rapidly from the smaller values to the larger values.
For the analysis of one step prediction for KOSPI 200 stock prices, the smoothness measure of (\ref{smooth}) is calculated
for every delay time $\tau$ between 1 and 20, and every embedding dimension $E$ between 2 and 10.
The calculated smoothness measure of KOSPI 200 stock prices is illustrated in Fig.~\ref{fig1}.
From the plot of smoothness measure, the smallest embedding dimension is selected first. Here, the proper
embedding dimension is selected as $E=3$. Then, for the given embedding dimension $E=3$, the proper delay time is
selected as $\tau=4$. As a result, the model of one step prediction is determined as
\begin{equation}
x(t+1) = f(x(t), x(t-4), x(t-8)).
\label{pr_model}
\end{equation}
From this phase space analysis, it is evident that the information of
3 data within 8 day window provides an important cue to determine the next day prediction
of KOSPI 200 stock price values.
\begin{figure}[t]
\centering \setcounter{subfigure}{0} \vspace{-1em}
\subfigure[]{\includegraphics[height=3.0cm]{Fig1}}
\subfigure[]{\includegraphics[height=4.0cm]{Fig2}}
\caption{The plot of smoothness measure for stock price prediction: (a) and (b) represent the 3D plot and contour map of stock price series. The dot in the contour map represents the selected delay time $\tau=4$ and embedding dimension $E=3$.}
\label{fig1}
\end{figure}

\section{Nonlinear Stock Price Prediction Model}

As a nonlinear prediction model for stock prices, a network with Gaussian kernel function is selected since this network
is able to perform nonlinear and nonparametric estimation and good for incremental learning due to the locality of kernel functions.
The suggested prediction model $\hat{f}$ with $m$ kernel functions is described by
\begin{equation}
\hat{f}(\vec{x})=\sum_{i=1}^mw_i\psi_i(\vec{x}), \;\;\; \psi_i(\vec{x}) = e^{-||\vec{x}-\vec{\mu}_i||^2/2\sigma_i^2},
\label{fhat}
\end{equation}
where $w_i$ represents the connection weight between the output and the $i$th kernel function $\psi_i$ in which
$\vec{\mu}_i$ and $\sigma_i$ represent the mean and standard deviation, respectively.
In (\ref{fhat}), we need to determine the parameters of $m$ and also kernel related parameters $w_i$, $\vec{\mu}_i$, $\sigma_i$.
Foe kernel related parameters, \cite{Kil93} suggested an efficient estimation method in such a way of minimizing
the mean square error (MSE). However, for our problem of prediction model, an optimal number $m$ of kernel functions should be determined to be fitted to
the stock price series so that the generalization error is minimized. Here, in the case of time series $x(t_k+P)$, it can be described by
\begin{equation}
x(t_k+P) = f(\vec{x}_{\tau,E}(k)) + \epsilon,
\end{equation}
where $f$ represent an embedded function for the generation of time series data and $\epsilon$ represents
a noise term expressed as a random variable with mean 0 and variance $\sigma^2$.

Then, for $x(t_k+P)$, the predicted value $\hat{x}(t_k+P)$ is described by
\begin{equation}
\hat{x}(t_k+P) = \hat{f}(\vec{x}_{\tau,E}(k)).
\end{equation}
The expected risk (or MSE) between the true and predicted values is described by
\begin{equation}
E[(x-\hat{x})^2] = E[(f - \hat{f})^2] + Var(\epsilon),
\label{risk}
\end{equation}
where $E[(f - \hat{f})^2]$ represents the regression error and  $Var(\epsilon)$ represents
the variance of noise.

From the above equation, it is clear that the expected risk is always greater than or equal to the variance of
noise. This implies that the empirical error (or training error) should not be less than the variance of noise
to avoid over-fitting of the regression model for the given time series. Here,  the variance of noise should be estimated.
One method of estimating the variance of noise is using the squares of time series differences \cite{rice}:
\begin{equation}
\hat{\sigma}^2={1\over{2(n-1)}}\sum_{k=1}^{n-1}(x(t_k) - x(t_{k-1}))^2,
\label{est_var}
\end{equation}
where $\hat{\sigma}^2$ represents an estimator of noise variance.

Assuming that the random noise follows a normal distribution, an estimator of (\ref{est_var}) follows
a chi-square distribution; that is,
\begin{equation}
{{(n-1)\hat{\sigma}^2}\over{\sigma^2}} \sim \chi_{n-1}^2.
\label{chi-sq}
\end{equation}
Then, with a confidence level of $1-\alpha$, the following probability of (\ref{chi-sq}) is described:
\begin{equation}
P\left( \chi_{1-\alpha/2, n-1}^2\le {{(n-1)\hat{\sigma}^2}\over{\sigma^2}}\le \chi_{\alpha/2, n-1}^2\right) = 1 - \alpha.
\end{equation}
From the above equation, a $100(1-\alpha)\%$ confidence interval for noise variance $\sigma^2$ is determined by
\begin{equation}
{{(n-1)\hat{\sigma}^2}\over{\chi_{\alpha/2, n-1}^2}}\le \sigma^2 \le {{(n-1)\hat{\sigma}^2}\over{\chi_{1-\alpha/2, n-1}^2}}.
\label{conf-int}
\end{equation}
In the case of large sample size, this interval is also effective even if the distribution of random noise is unknown because both
an estimator of (\ref{est_var}) and chi-square random variable approximately follow a normal distribution for large $n$ according to
the central limit theorem. Then, from (\ref{risk}) and (\ref{conf-int}), it is recommended that the training error should not be
within the confidence interval for noise variance. From this point of view, the following algorithm of constructing a stock price prediction model is suggested:

\vspace{1em}

\underline{Construction of Nonlinear Stock Price Prediction Model}
\begin{description}
\item[Step 1.] From the given stock price series $x(t_k)$, $k=0, 1, \cdots, n-1$ and prediction time $P$,
                      determine the values of smoothness measure of (\ref{smooth}).
                      Then, determine the embedding dimension $E$ and delay time $\tau$ when the value of smoothness measure is large (usually, greater than -1)
                      at the smaller embedding dimension.
\item[Step 2.] Determine the structure of nonlinear prediction model of (\ref{target:disc}).
\item[Step 3.] Estimate the noise variance using (\ref{est_var}) and determine a $100(1-\alpha)\%$ confidence interval for noise variance
                      using (\ref{conf-int}).
\item[Step 4.] Train the nonlinear prediction model using an incremental learning algorithm such as \cite{Kil93} as the number of kernel functions increases
                      and determine the following training error:
\begin{equation}
R_{emp}(\hat{f})={1\over n}\sum_{k=0}^{n-1}(x(t_k)-\hat{x}(t_k))^2.
\end{equation}
\item[Step 5.] If the condition
\begin{equation}
R_{emp}(\hat{f})>{{(n-1)\hat{\sigma}^2}\over{\chi_{1-\alpha/2, n-1}^2}}
\end{equation}
                       is met, continue the learning process of Step 4. Otherwise, stop the learning process.
\end{description}
This construction algorithm was applied to KOSPI 200 stock price series using the prediction model of (\ref{pr_model}).
In this experiment, an 80\% of stock price series was used as training data and the remaining 20\% was used as test data.
Here, the noise variance was estimated as $\hat{\sigma}^2=0.000127$ and a 99\% confidence interval for noise variance $\sigma^2$ was
determined by
\[
0.000115\le \sigma^2\le 0.000159.
\]
Here, to compare with the training and test error with respect to the number of kernel functions,
the training error, test error, and confidence interval were plotted as illustrated in Fig.~\ref{fig3}.

\begin{figure}
\vspace{-1em}
\centering
\includegraphics[height=6.2cm]{Fig3b}
\vspace{-1em}
\caption{Selection of the number of kernel functions by comparing the training error with the confidence interval for noise variance}
\label{fig3}
\end{figure}

As shown in Fig.~\ref{fig3}, the over-fitting of regression model occurs when the number of kernel function is greater than or
equal to 54. In this case, the training error is within the confidence interval for noise variance. Therefore, in this experiment,
it is clear that the proper number of kernel functions is less than or equal to 42 which makes the training error just greater than
the upper limit of the confidence interval.

\section{Simulation}

For the simulation for stock price prediction, the data set of  Korea Composite Stock Price Index (KOSPI) 200 stock price series from June 2008 to June 2012 was
selected and normalized by dividing the price values by 200 so that approximate price range was between 0.5 and 1.5. In this data set, the first 70\%, 80\%, and 90\%
of data were used as training data sets, and the remaining data sets were used as test data sets. First, to identify the dynamics of stock price series for the given
prediction time $P=1$, the embedding dimension $E$ and delay time $\tau$ were determined using the smoothness measure of (\ref{smooth}). As a result, the prediction
model of (\ref{pr_model}) was obtained. Then, the noise variance was estimated using the estimator of (\ref{est_var}) and a 99\% confidence interval
for noise variance was determined using the form of (\ref{conf-int}). For the given prediction model, the parameters of Gaussian kernel function network (GKFN) of (\ref{fhat})
were trained using the nonparametric estimation method of \cite{Kil93} as the number of kernel functions increased. This training was continuously performed
until the training error reached the upper limit of the confidence interval for noise variance. Here, to measure the performances of stock price prediction, the following root mean square error
(RMSE) and the coefficient of determination $R^2$ were use for the test data $y(t_i)$, $i= 0, \cdots, l-1$:
\begin{equation}
RMSE = \left({1\over l}\sum_{i=0}^{l-1}(y(t_i) - \hat{y}(t_i))^2\right)^{1/2},
\end{equation}
where $l$ represents the number of test data and
\begin{equation}
R^2 = 1 - \left(\sum_{l=0}^{l-1}(y(t_i) - \hat{y}(t_i))^2/\sum_{l=0}^{l-1}(y(t_i) - \bar{y})^2\right),
\end{equation}
where $\bar{y}$ represent the sample mean of $y(t_i)$; that is, $\bar{y} = (1/l)\sum_{l=0}^{l-1}y(t_i)$.
Here, the range of $R^2$ is between 0 and 1, and $R^2$ indicates the degree of how well the regression model
covers the variation of time series data; for example, if $R^2 = 0.9$, the regression model covers 90\% of
the variation of time series data and the remaining 10\% of the variation is not explained.
Then, the simulation results for stock price prediction using the proposed GKFN were obtained and described in Table~\ref{sim_res}.
For the comparison, simulation results for stock price prediction using the kernel support vector machine (k-SVM) \cite{svmlight},
one of popular methods in nonlinear regression models, were also obtained. In the k-SVM, the same form of
the prediction model of (\ref{pr_model}) was also used.

\begin{table}
\caption{Simulation results for stock price prediction}
\label{sim_res}
\centering
\begin{tabular}{||p{2cm}||p{1.6cm}|p{1.6cm}||p{1.6cm}|p{1.6cm}||}\hline
Ratio of        & \multicolumn{2}{c||}{k-SVM} & \multicolumn{2}{c||}{GKFN}\\ \cline{2-5}
training data & RMSE    & $R^2$ & RMSE             & $R^2$ \\ \hline
70\%          & 0.058423 & 0.60    & {\bf 0.035879} & {\bf 0.87} \\ \hline
80\%          & 0.039960 & 0.64    & {\bf 0.020517} & {\bf 0.90} \\ \hline
90\%          & 0.036820 & 0.41    & {\bf 0.016172} & {\bf 0.88} \\ \hline
\end{tabular}
\end{table}
These simulation results showed that the RMSE was decreased as the ratio of training data was increased and
the proposed GKFN outperformed the k-SVM from the view points of the RMSE and also $R^2$. In general, if
$R^2$ is greater than or equal to 0.9, we consider that the regression model performs very good fit to the given data.
The proposed GKFN was very close to this performance. The main reason was due to the fact that the prediction model
was determined by phase space analysis and the optimization of regression model was done by comparing
the training error with the noise variance estimate.

\section{Conclusion}

The stock price prediction involves the analysis of stock price series and also optimization of
regression models. In this work, the stock price series is analyzed by the phase space analysis
method \cite{Kil99}. As the prediction model, a network with Gaussian kernel functions is selected since this model is
good for incremental learning due to the locality of kernel functions. For the optimization of
regression model, the noise variance is estimated and used to determine the proper number of
kernel functions. As a result, the proposed model provides the high accuracy of predicted values.
Through the simulation for the prediction of KOSPI 200 stock price values, the effectiveness
of the proposed prediction model has been demonstrated.
The proposed model can also be applied to various problems of time
series prediction.

\begin{thebibliography}{4}

\bibitem{peter}
Peters, E. E.: Fractal market analysis: applying chaos theory to investment and economics.
John Wiley \& Sons, New York, NY (1994)

\bibitem{Packard80}
Packard, N. H., Crutchfield, J.P., Farmer J. D., and Shaw, R. S.: Geometry
from a Time Series. Phys. Rev. Lett., vol. 45, 712--716 (1980)

\bibitem{Takens81}
Takens, F.: Detecting strange attractors in turbulence. In: Rand,
D. A. and Young, L. S. (eds.): Dynamical Systems and Turbulence, Warwick
1980. Lecture Notes in Mathematics, vol. 898. Springer-Verlag,
Berlin 366--381 (1981)

\bibitem{Havstad89}
Havstad, J. W. and Ehlers, C. L.: Attractor dimension of nonstationary
dynamical systems from small data sets. Phys. Rev. A
vol. 39, no. 2, 845--853 (1989)

\bibitem{Kil99}
Kil, R., Park, S., and Kim, S.: Time series analysis based on the smoothness measure of
mapping in the phse space of attractors. International Joint Conference on Neural Networks,
vol. 4, 2584--2589 (1999)

\bibitem{Kil93}
Kil, R.: Function approximation based on a network with kernel
functions of bounds and locality. ETRI journal, vol. 15, 35--51 (1993)

\bibitem{rice}
Rice, J.: Bandwidth choice for nonparametric regression. Annals of Statistics, vol. 12, no. 4, 1215--1230 (1984)

\bibitem{svmlight}
SVMlight ver 6.02, Cornell University, {\tt http://svmlight.joachims.org}

\end{thebibliography}

\end{document}
